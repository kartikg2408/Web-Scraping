{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Read video URLs from the CSV file\n",
    "api_key = 'AIzaSyAy5zvDfg9AWSOVKYBnU-NkJfVtun5hKH0'  # Replace with your YouTube Data API key\n",
    "\n",
    "# Create a YouTube Data API client\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "def clean_comments(comment):\n",
    "    # Remove URLs, special characters, emojis, and other noise\n",
    "    cleaned_comment = re.sub(r'http\\S+', '', comment)  # Remove URLs\n",
    "    cleaned_comment = re.sub(r'[^\\w\\s]', '', cleaned_comment)  # Remove special characters\n",
    "    cleaned_comment = re.sub(r'[^a-zA-Z0-9]', ' ', cleaned_comment)  # Remove non-alphanumeric characters\n",
    "    cleaned_comment = re.sub(r'(?<=\\s)@\\w+', '', cleaned_comment)  # Remove mentions\n",
    "    cleaned_comment = re.sub(r'[^\\x00-\\x7F]+', '', cleaned_comment)  # Remove non-ASCII characters\n",
    "    \n",
    "    # Convert to lowercase and remove extra spaces\n",
    "    cleaned_comment = cleaned_comment.lower().strip()\n",
    "    \n",
    "    return cleaned_comment\n",
    "\n",
    "video_urls = []\n",
    "with open('/Users/akhilsharma/Documents/jupyter/video.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        video_urls.append(row['VideoURL'])\n",
    "\n",
    "video_data = []\n",
    "\n",
    "# Loop through each video URL and get information\n",
    "for idx, video_url in enumerate(video_urls, start=1):\n",
    "    try:\n",
    "        # Extract video ID from the URL\n",
    "        video_id = video_url.split('v=')[-1]\n",
    "        \n",
    "        # Use YouTube Data API to get video details\n",
    "        video_response = youtube.videos().list(\n",
    "            part='snippet,statistics',\n",
    "            id=video_id\n",
    "        ).execute()\n",
    "        \n",
    "        video_snippet = video_response['items'][0]['snippet']\n",
    "        video_stats = video_response['items'][0]['statistics']\n",
    "        \n",
    "        # Fetch comments using YouTube Data API\n",
    "        comment_list = []\n",
    "        next_page_token = None\n",
    "        \n",
    "        while True:\n",
    "            comments_response = youtube.commentThreads().list(\n",
    "                part='snippet',\n",
    "                videoId=video_id,\n",
    "                textFormat='plainText',\n",
    "                maxResults=150,\n",
    "                pageToken=next_page_token\n",
    "            ).execute()\n",
    "            \n",
    "            for comment_item in comments_response['items']:\n",
    "                comment = comment_item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                cleaned_comment = clean_comments(comment)\n",
    "                comment_list.append(cleaned_comment)\n",
    "                \n",
    "            next_page_token = comments_response.get('nextPageToken')\n",
    "            if not next_page_token or len(comment_list) >= 150:\n",
    "                break  # No more pages available or reached 150 comments\n",
    "        \n",
    "        comments_text = ' // '.join(comment_list)  # Use ' // ' as delimiter\n",
    "        \n",
    "        video_data.append({\n",
    "            'Index': idx,\n",
    "            'Video Url': video_url,\n",
    "            '': '', \n",
    "            'Video Title': video_snippet['title'],\n",
    "            'Video Views': video_stats.get('viewCount', 0),\n",
    "            'Video Likes': video_stats.get('likeCount', 0),\n",
    "            'Video Comments': video_stats.get('commentCount', 0),\n",
    "            'Video Author': video_snippet['channelTitle'],\n",
    "            'Comments': comments_text  # Use the concatenated comments_text\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {video_url}: {e}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "output_csv_file = 'clean_comment.csv'\n",
    "with open(output_csv_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    fieldnames = ['Index', 'Video Url', '', 'Video Title', 'Video Views', 'Video Likes', 'Video Comments', 'Video Author', 'Comments']\n",
    "    csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "    for data in video_data:\n",
    "        # Encode each field, ignoring problematic characters\n",
    "        encoded_data = {k: v.encode('utf-8', 'ignore').decode('utf-8') if isinstance(v, str) else v for k, v in data.items()}\n",
    "        csv_writer.writerow(encoded_data)\n",
    "\n",
    "print(f\"Video data saved  with clean commnets\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
